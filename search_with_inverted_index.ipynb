{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = pd.read_json(\"processed_text.json\")\n",
    "processed_text_tf_idf = pd.read_json(\"processed_text_tf_idf.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>tfidf_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pandemic</td>\n",
       "      <td>A pandemic (from Greek πᾶν, pan, \"all\" and δῆμ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Pandemic</td>\n",
       "      <td>[pandemic, greek, pan, demo, people, epidemic,...</td>\n",
       "      <td>{'pandemic': 0.0504098705, 'greek': 3.25809653...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Epidemiology of HIV/AIDS</td>\n",
       "      <td>HIV/AIDS, or Human Immunodeficiency Virus, is ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Epidemiology_of_...</td>\n",
       "      <td>[hiv, aids, human, immunodeficiency, virus, co...</td>\n",
       "      <td>{'pandemic': 0.0144028201, 'greek': 0.0, 'pan'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Antonine Plague</td>\n",
       "      <td>The Antonine Plague of 165 to 180 AD, also kno...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Antonine_Plague</td>\n",
       "      <td>[plague, ad, know, plague, galen, galen, physi...</td>\n",
       "      <td>{'pandemic': 0.0144028201, 'greek': 0.0, 'pan'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basic reproduction number</td>\n",
       "      <td>In epidemiology, the basic reproduction number...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Basic_reproducti...</td>\n",
       "      <td>[epidemiology, basic, reproduction, number, ba...</td>\n",
       "      <td>{'pandemic': 0.0, 'greek': 0.0, 'pan': 0.0, 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bills of mortality</td>\n",
       "      <td>Bills of mortality were the weekly mortality s...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bills_of_mortality</td>\n",
       "      <td>[bill, mortality, weekly, mortality, statistic...</td>\n",
       "      <td>{'pandemic': 0.0, 'greek': 0.0, 'pan': 0.0, 'd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title  \\\n",
       "0                   Pandemic   \n",
       "1   Epidemiology of HIV/AIDS   \n",
       "2            Antonine Plague   \n",
       "3  Basic reproduction number   \n",
       "4         Bills of mortality   \n",
       "\n",
       "                                                text  \\\n",
       "0  A pandemic (from Greek πᾶν, pan, \"all\" and δῆμ...   \n",
       "1  HIV/AIDS, or Human Immunodeficiency Virus, is ...   \n",
       "2  The Antonine Plague of 165 to 180 AD, also kno...   \n",
       "3  In epidemiology, the basic reproduction number...   \n",
       "4  Bills of mortality were the weekly mortality s...   \n",
       "\n",
       "                                                 url  \\\n",
       "0             https://en.wikipedia.org/wiki/Pandemic   \n",
       "1  https://en.wikipedia.org/wiki/Epidemiology_of_...   \n",
       "2      https://en.wikipedia.org/wiki/Antonine_Plague   \n",
       "3  https://en.wikipedia.org/wiki/Basic_reproducti...   \n",
       "4   https://en.wikipedia.org/wiki/Bills_of_mortality   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  [pandemic, greek, pan, demo, people, epidemic,...   \n",
       "1  [hiv, aids, human, immunodeficiency, virus, co...   \n",
       "2  [plague, ad, know, plague, galen, galen, physi...   \n",
       "3  [epidemiology, basic, reproduction, number, ba...   \n",
       "4  [bill, mortality, weekly, mortality, statistic...   \n",
       "\n",
       "                                        tfidf_vector  \n",
       "0  {'pandemic': 0.0504098705, 'greek': 3.25809653...  \n",
       "1  {'pandemic': 0.0144028201, 'greek': 0.0, 'pan'...  \n",
       "2  {'pandemic': 0.0144028201, 'greek': 0.0, 'pan'...  \n",
       "3  {'pandemic': 0.0, 'greek': 0.0, 'pan': 0.0, 'd...  \n",
       "4  {'pandemic': 0.0, 'greek': 0.0, 'pan': 0.0, 'd...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text_tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary \n",
    "# vocab dict contains 'cornovirus' but the lemmatized verion is turning into 'coronaviru'\n",
    "vocab = set([word for document in processed_text_tf_idf.processed_text for word in document])\n",
    "vocab_dict = {word:[] for word in vocab}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, document_idf in enumerate(processed_text_tf_idf.tfidf_vector):\n",
    "    for key, value in document_idf.items():\n",
    "        if value > 0:\n",
    "            vocab_dict[key].append((processed_text.title[idx], value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('COVID-19 pandemic', 3.258096538)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"coronaviru\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a search funciton\n",
    "# 1. Tokenize the new phrase\n",
    "# 2. Filter to words in the dictionary that are in the phrase\n",
    "# 3. Sum grouped by the documents to get the highest tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_query_tokens(text, vocab = vocab):\n",
    "    \"\"\" Accepts any list of words and returns a tokenized list\n",
    "\n",
    "    Args:\n",
    "        text (string):\n",
    "\n",
    "    Returns:\n",
    "        list: Tokenizeed list of lemmatized words\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_lg\") # Initialize the vocabulary\n",
    "    doc = nlp(text.lower())\n",
    "    filtered_sentence =[] \n",
    "    for word in doc:\n",
    "        lexeme = nlp.vocab[str(word)]\n",
    "        if lexeme.is_stop == False and lexeme.is_punct == False and lexeme.is_oov == False and lexeme.lower_ in vocab:\n",
    "            filtered_sentence.append(word.lemma_) \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coronaviru']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_search_term = make_query_tokens(\"coronavirus\")\n",
    "tokenized_search_term\n",
    "# Need to implment fix here to check if search word in vocab\n",
    "#relavent_documents_dictionary = {token:vocab_dict[token] for token in tokenized_search_term}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tf_idf_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\TFACKR~1\\AppData\\Local\\Temp/ipykernel_8784/4097132024.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pd.DataFrame([doc for word in list(relavent_documents_dictionary.values()) for doc in word],\n\u001b[0m\u001b[0;32m      2\u001b[0m              columns=[\"title\", \"tf_idf_score\"]).groupby(\"title\").sum().sort_values(by = 'tf_idf_score', ascending = False)\n",
      "\u001b[1;32mc:\\Users\\tfackrell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tfackrell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6258\u001b[0m             \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6259\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6261\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tfackrell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1777\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1778\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1779\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tf_idf_score'"
     ]
    }
   ],
   "source": [
    "pd.DataFrame([doc for word in list(relavent_documents_dictionary.values()) for doc in word],\n",
    "             columns=[\"title\", \"tf_idf_score\"]).groupby(\"title\").sum().sort_values(by = 'tf_idf_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index_search(search_string:str, inverted_index = vocab_dict):\n",
    "    \"\"\"Accept a search string and an inverted index, return the most sorted search results in teh form of a dataframe\n",
    "\n",
    "    Args:\n",
    "        search_string (str): _description_\n",
    "        inverted_index (_type_): _description_\n",
    "    \"\"\"\n",
    "    tokenized_search_term = make_query_tokens(search_string)\n",
    "    relavent_documents_dictionary = {token:vocab_dict[token] for token in tokenized_search_term}\n",
    "    \n",
    "    search_results = pd.DataFrame([doc for word in list(relavent_documents_dictionary.values()) for doc in word],\n",
    "             columns=[\"title\", \"tf_idf_score\"]).groupby(\"title\").sum().sort_values(by = 'tf_idf_score', ascending = False)\n",
    "    \n",
    "    return search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tf_idf_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\TFACKR~1\\AppData\\Local\\Temp/ipykernel_8784/4138418984.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minverted_index_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corona\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\TFACKR~1\\AppData\\Local\\Temp/ipykernel_8784/2079065144.py\u001b[0m in \u001b[0;36minverted_index_search\u001b[1;34m(search_string, inverted_index)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mrelavent_documents_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_search_term\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     search_results = pd.DataFrame([doc for word in list(relavent_documents_dictionary.values()) for doc in word],\n\u001b[0m\u001b[0;32m     12\u001b[0m              columns=[\"title\", \"tf_idf_score\"]).groupby(\"title\").sum().sort_values(by = 'tf_idf_score', ascending = False)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tfackrell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tfackrell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6258\u001b[0m             \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6259\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6261\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tfackrell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1777\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1778\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1779\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tf_idf_score'"
     ]
    }
   ],
   "source": [
    "inverted_index_search(\"corona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce6b2e46da96f80237154c4d39870d1180913a52ccea59b3717ca4730176c7ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
