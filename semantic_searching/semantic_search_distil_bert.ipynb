{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use AutoTokenizer and AutoModel classes from Transformers library to load a pre-trained model from Transformers, along with the appropriate tokenizer.\n",
    "https://huggingface.co/docs/transformers/model_doc/auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_json(\"https://lp-prod-resources.s3.amazonaws.com/493/57248/2021-05-04-13-31-46/sentences.json\").rename(columns = {0:\"sentence_text\"}) \n",
    "questions = pd.read_json(\"https://lp-prod-resources.s3.amazonaws.com/493/57248/2021-08-16-19-04-45/questions.json\").rename(columns = {0:\"question_text\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                        sentence_text\n",
       " 0   A pandemic is an epidemic of an infectious dis...\n",
       " 1   The most fatal pandemic in recorded history wa...\n",
       " 2   Current pandemics include COVID-19 (SARS-CoV-2...\n",
       " 3   As of 2018, approximately 37.9 million people ...\n",
       " 4   Cholera is an infection of the small intestine...\n",
       " 5   Classic cholera symptom is large amounts of wa...\n",
       " 6   The COVID-19 pandemic, also known as the coron...\n",
       " 7   Common symptoms of COVID-19 include fever, cou...\n",
       " 8   The Plague of Cyprian was a pandemic that affl...\n",
       " 9   The Spanish flu, also known as the 1918 flu pa...\n",
       " 10  The death toll of Spanish Flu is estimated to ...,\n",
       "                                        question_text\n",
       " 0      How many people have died during Black Death?\n",
       " 1      Which diseases can be transmitted by animals?\n",
       " 2  Connection between climate change and a likeli...\n",
       " 3               What is an example of a latent virus\n",
       " 4                          Viruses in nanotechnology\n",
       " 5                       Giant viruses classification\n",
       " 6  What are the notable pandemic prevention organ...\n",
       " 7    How many leprosy outbreaks are known to happen?\n",
       " 8  What are the geographic areas with the highest...\n",
       " 9     How to prevent the spread of viral infections?)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 1.45MB/s]\n",
      "c:\\Users\\tanne\\Documents\\nlp_tutorial\\nlp_elastic-search\\64_bit_env\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tanne\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 28.0kB/s]\n",
      "Downloading: 100%|██████████| 483/483 [00:00<00:00, 242kB/s]\n",
      "Downloading: 100%|██████████| 268M/268M [00:39<00:00, 6.85MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sentences[\"sentence_text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the documents into tensors\n",
    "vectors = [\n",
    "    model(**tokenizer(document, return_tensors = 'pt'))[0].detach().squeeze()\n",
    "    for document in documents\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([35, 768]),\n",
       " torch.Size([37, 768]),\n",
       " torch.Size([25, 768]),\n",
       " torch.Size([18, 768]),\n",
       " torch.Size([24, 768]),\n",
       " torch.Size([55, 768]),\n",
       " torch.Size([57, 768]),\n",
       " torch.Size([24, 768]),\n",
       " torch.Size([27, 768]),\n",
       " torch.Size([35, 768]),\n",
       " torch.Size([43, 768])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the sizes\n",
    "[v.size() for v in vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average on dimension 0 to create uniform sizes (These match the orginal size of the faiss index 768 also happens to be the hidden ouput of distilbert)\n",
    "\n",
    "averaged_vectors = [torch.mean(vector, dim=0) for vector in vectors]\n",
    "[v.size() for v in averaged_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an encoding function to turn a string into a tensor\n",
    "def encode(document: str) -> torch.Tensor:\n",
    "  tokens = tokenizer(document, return_tensors='pt')\n",
    "  vector = model(**tokens)[0].detach().squeeze()\n",
    "  return torch.mean(vector, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) # the size of our vector space (Also the size of the hidden ouput of distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index all the documents, we need to get the numpy array for each document\n",
    "index.add_with_ids(\n",
    "    np.array([t.numpy() for t in averaged_vectors]),\n",
    "    # the IDs will be 0 to len(documents)\n",
    "    np.array(range(0, len(documents))).astype(np.int64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our search funciton will receive a string, encode it, and use the encoded tensor to search the index returning the k most similar documents\n",
    "def search(query: str, k=1):\n",
    "  encoded_query = encode(query).unsqueeze(dim=0).numpy()\n",
    "  top_k = index.search(encoded_query, k)\n",
    "  scores = top_k[0][0]\n",
    "  results = [documents[_id] for _id in top_k[1][0]]\n",
    "  return list(zip(results, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which diseases can be transmitted by animals?\n",
      "Answer: [('A pandemic is an epidemic of an infectious disease that has spread across a large region, for instance multiple continents or worldwide, affecting a substantial number of people.', 54.0495), ('Cholera is an infection of the small intestine by some strains of the bacterium Vibrio cholerae.', 50.804134), ('Current pandemics include COVID-19 (SARS-CoV-2) and HIV/AIDS.', 50.652287), ('As of 2018, approximately 37.9 million people are infected with HIV globally.', 50.516018), ('The Spanish flu, also known as the 1918 flu pandemic, was an unusually deadly influenza pandemic caused by the H1N1 influenza A virus.', 48.97045)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_num = 1\n",
    "num_similar_docs = 5\n",
    "\n",
    "print(\"Question: {}\".format(questions.question_text[q_num])), print(\"Answer: {}\".format(search(questions.question_text[q_num], num_similar_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('64_bit_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91ca92bfb3efb2ea76f4cb03a3cae8a17e697e793a68c1f48db45dcdde3388a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
